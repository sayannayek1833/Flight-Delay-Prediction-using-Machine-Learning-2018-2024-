{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bbb9d69",
   "metadata": {},
   "source": [
    "# 04 - Model Training (Baseline & Tree-Based Models)\n",
    "\n",
    "In this notebook we train baseline models and tree-based models for flight delay prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f84e43-6197-4ac1-b37a-44c19ad5026d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (582425, 51)\n",
      "Feature shape: (582425, 45)\n",
      "Target distribution:\n",
      " DELAYED\n",
      "0    0.616778\n",
      "1    0.383222\n",
      "Name: proportion, dtype: float64\n",
      "Train shape: (465940, 45) Test shape: (116485, 45)\n",
      "Categorical columns: ['Marketing_Airline_Network', 'Operated_or_Branded_Code_Share_Partners', 'IATA_Code_Marketing_Airline', 'Operating_Airline', 'IATA_Code_Operating_Airline', 'Origin', 'OriginCityName', 'OriginState', 'OriginStateName', 'Dest', 'DestCityName', 'DestState', 'DestStateName', 'DepTimeBlk', 'DistanceGroup', 'DepPartOfDay', 'Route', 'StateRoute', 'Airline']\n",
      "Numerical columns: ['Year', 'Quarter', 'Month', 'DayofMonth', 'DayOfWeek', 'DOT_ID_Marketing_Airline', 'Flight_Number_Marketing_Airline', 'DOT_ID_Operating_Airline', 'Flight_Number_Operating_Airline', 'OriginAirportID', 'OriginAirportSeqID', 'OriginCityMarketID', 'OriginStateFips', 'OriginWac', 'DestAirportID', 'DestAirportSeqID', 'DestCityMarketID', 'DestStateFips', 'DestWac', 'CRSDepTime', 'Cancelled', 'Diverted', 'CRSElapsedTime', 'Flights', 'Distance', 'DepHour']\n",
      "\n",
      "Fitting Logistic Regression (baseline)...\n",
      "\n",
      "===== Logistic Regression Performance =====\n",
      "Accuracy : 0.6636305103661415\n",
      "Precision: 0.5899117026884555\n",
      "Recall   : 0.40109767025089604\n",
      "F1-score : 0.4775176018775336\n",
      "ROC-AUC  : 0.6930472641097385\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.83      0.75     71845\n",
      "           1       0.59      0.40      0.48     44640\n",
      "\n",
      "    accuracy                           0.66    116485\n",
      "   macro avg       0.64      0.61      0.61    116485\n",
      "weighted avg       0.65      0.66      0.65    116485\n",
      "\n",
      "\n",
      "Low-cardinality categorical columns for HGB: ['Marketing_Airline_Network', 'Operated_or_Branded_Code_Share_Partners', 'IATA_Code_Marketing_Airline', 'DepTimeBlk', 'DistanceGroup', 'DepPartOfDay']\n",
      "\n",
      "Fitting HistGradientBoostingClassifier on reduced feature set...\n",
      "\n",
      "===== HistGradientBoosting (reduced features) Performance =====\n",
      "Accuracy : 0.7100399193029145\n",
      "Precision: 0.6732474325444919\n",
      "Recall   : 0.47287186379928314\n",
      "F1-score : 0.5555438587256889\n",
      "ROC-AUC  : 0.7579979374592006\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.86      0.78     71845\n",
      "           1       0.67      0.47      0.56     44640\n",
      "\n",
      "    accuracy                           0.71    116485\n",
      "   macro avg       0.70      0.67      0.67    116485\n",
      "weighted avg       0.70      0.71      0.70    116485\n",
      "\n",
      "\n",
      "Saved best model to models/best_flight_delay_model.pkl\n"
     ]
    }
   ],
   "source": [
    "# 04_model_training.ipynb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    classification_report,\n",
    ")\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "\n",
    "# ======================\n",
    "# 1. LOAD ENGINEERED DATA\n",
    "# ======================\n",
    "\n",
    "data_path = \"data/flight_data_2018_2024_engineered.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "print(\"Data shape:\", df.shape)\n",
    "\n",
    "# ======================\n",
    "# 2. DEFINE TARGET & DROP NON-PREDICTIVE COLUMNS\n",
    "# ======================\n",
    "\n",
    "target = \"DELAYED\"\n",
    "\n",
    "cols_to_remove = [\n",
    "    \"DELAYED\",\n",
    "    \"FlightDate\",\n",
    "    \"Duplicate\",\n",
    "    \"DivAirportLandings\",\n",
    "    \"CRSArrTime\",\n",
    "    \"ArrTimeBlk\",\n",
    "]\n",
    "\n",
    "cols_to_drop_final = [c for c in cols_to_remove if c in df.columns]\n",
    "\n",
    "X = df.drop(columns=cols_to_drop_final)\n",
    "y = df[target]\n",
    "\n",
    "print(\"Feature shape:\", X.shape)\n",
    "print(\"Target distribution:\\n\", y.value_counts(normalize=True))\n",
    "\n",
    "# ======================\n",
    "# 3. TRAIN–TEST SPLIT\n",
    "# ======================\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\n",
    "\n",
    "# ======================\n",
    "# 4. IDENTIFY COLUMN TYPES\n",
    "# ======================\n",
    "\n",
    "cat_cols = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "num_cols = X.select_dtypes(exclude=\"object\").columns.tolist()\n",
    "\n",
    "print(\"Categorical columns:\", cat_cols)\n",
    "print(\"Numerical columns:\", num_cols)\n",
    "\n",
    "# Downcast numeric to float32 to save memory\n",
    "X_train[num_cols] = X_train[num_cols].astype(np.float32)\n",
    "X_test[num_cols] = X_test[num_cols].astype(np.float32)\n",
    "\n",
    "# ======================\n",
    "# 5. PREPROCESSOR FOR LOGISTIC (SPARSE, FULL FEATURES)\n",
    "# ======================\n",
    "\n",
    "preprocessor_logit = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=True), cat_cols),\n",
    "        (\"num\", StandardScaler(), num_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ======================\n",
    "# 6. EVALUATION FUNCTION\n",
    "# ======================\n",
    "\n",
    "def evaluate_model(name, y_true, y_pred, y_prob):\n",
    "    print(f\"\\n===== {name} Performance =====\")\n",
    "    print(\"Accuracy :\", accuracy_score(y_true, y_pred))\n",
    "    print(\"Precision:\", precision_score(y_true, y_pred))\n",
    "    print(\"Recall   :\", recall_score(y_true, y_pred))\n",
    "    print(\"F1-score :\", f1_score(y_true, y_pred))\n",
    "    print(\"ROC-AUC  :\", roc_auc_score(y_true, y_prob))\n",
    "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))\n",
    "\n",
    "# ======================\n",
    "# 7. LOGISTIC REGRESSION (BASELINE, SAME AS BEFORE)\n",
    "# ======================\n",
    "\n",
    "log_reg_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor_logit),\n",
    "    (\"model\", LogisticRegression(max_iter=3000, n_jobs=-1)),\n",
    "])\n",
    "\n",
    "print(\"\\nFitting Logistic Regression (baseline)...\")\n",
    "log_reg_pipeline.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr = log_reg_pipeline.predict(X_test)\n",
    "y_prob_lr = log_reg_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "evaluate_model(\"Logistic Regression\", y_test, y_pred_lr, y_prob_lr)\n",
    "\n",
    "# ======================\n",
    "# 8. PREPROCESSOR FOR HIST GRADIENT BOOSTING (MEMORY-SAFE)\n",
    "# ======================\n",
    "\n",
    "# Use only low-cardinality categorical features for HGB\n",
    "# (to avoid huge dense one-hot matrices)\n",
    "low_card_cats = [c for c in cat_cols if df[c].nunique() <= 20]\n",
    "\n",
    "print(\"\\nLow-cardinality categorical columns for HGB:\", low_card_cats)\n",
    "\n",
    "# High-cardinality features like Route, StateRoute, city names etc.\n",
    "# are DROPPED for HGB to avoid memory explosion.\n",
    "hgb_feature_cols = num_cols + low_card_cats\n",
    "\n",
    "X_train_hgb = X_train[hgb_feature_cols].copy()\n",
    "X_test_hgb = X_test[hgb_feature_cols].copy()\n",
    "\n",
    "preprocessor_hgb = ColumnTransformer(\n",
    "    transformers=[\n",
    "        # Few low-cardinality cats → dense one-hot is fine\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), low_card_cats),\n",
    "        # Tree-based models don't need scaling; pass numeric as-is\n",
    "        (\"num\", \"passthrough\", num_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# ======================\n",
    "# 9. HIST GRADIENT BOOSTING (FAST, STRONG MODEL)\n",
    "# ======================\n",
    "\n",
    "hgb_pipeline = Pipeline(steps=[\n",
    "    (\"preprocessor\", preprocessor_hgb),\n",
    "    (\"model\", HistGradientBoostingClassifier(\n",
    "        max_depth=10,\n",
    "        learning_rate=0.1,\n",
    "        max_iter=200,\n",
    "        random_state=42\n",
    "    )),\n",
    "])\n",
    "\n",
    "print(\"\\nFitting HistGradientBoostingClassifier on reduced feature set...\")\n",
    "hgb_pipeline.fit(X_train_hgb, y_train)\n",
    "\n",
    "y_pred_hgb = hgb_pipeline.predict(X_test_hgb)\n",
    "y_prob_hgb = hgb_pipeline.predict_proba(X_test_hgb)[:, 1]\n",
    "\n",
    "evaluate_model(\"HistGradientBoosting (reduced features)\", y_test, y_pred_hgb, y_prob_hgb)\n",
    "\n",
    "# ======================\n",
    "# 10. ADDITIONAL MODELS - XGBOOST, SVC, KNN, DECISION TREE\n",
    "# ======================\n",
    "\n",
    "# Train and evaluate additional models\n",
    "additional_models = {\n",
    "    \"XGBoost\": XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss'),\n",
    "    \"SVC\": SVC(random_state=42, probability=True),\n",
    "    \"KNeighbors\": KNeighborsClassifier(n_jobs=-1),\n",
    "    \"DecisionTree\": DecisionTreeClassifier(random_state=42)\n",
    "}\n",
    "\n",
    "model_results = {\n",
    "    \"Logistic Regression\": {\n",
    "        \"y_pred\": y_pred_lr,\n",
    "        \"y_prob\": y_prob_lr,\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred_lr),\n",
    "        \"f1\": f1_score(y_test, y_pred_lr),\n",
    "        \"roc_auc\": roc_auc_score(y_test, y_prob_lr)\n",
    "    },\n",
    "    \"HistGradientBoosting\": {\n",
    "        \"y_pred\": y_pred_hgb,\n",
    "        \"y_prob\": y_prob_hgb,\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred_hgb),\n",
    "        \"f1\": f1_score(y_test, y_pred_hgb),\n",
    "        \"roc_auc\": roc_auc_score(y_test, y_prob_hgb)\n",
    "    }\n",
    "}\n",
    "\n",
    "for name, model in additional_models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {name}...\")\n",
    "    print('='*60)\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = Pipeline([\n",
    "        (\"preprocessor\", preprocessor_hgb),\n",
    "        (\"model\", model)\n",
    "    ])\n",
    "    \n",
    "    # Train\n",
    "    pipeline.fit(X_train_hgb, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = pipeline.predict(X_test_hgb)\n",
    "    y_prob = pipeline.predict_proba(X_test_hgb)[:, 1]\n",
    "    \n",
    "    # Evaluate\n",
    "    evaluate_model(name, y_test, y_pred, y_prob)\n",
    "    \n",
    "    # Store results\n",
    "    model_results[name] = {\n",
    "        \"y_pred\": y_pred,\n",
    "        \"y_prob\": y_prob,\n",
    "        \"accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"f1\": f1_score(y_test, y_pred),\n",
    "        \"roc_auc\": roc_auc_score(y_test, y_prob)\n",
    "    }\n",
    "\n",
    "# ======================\n",
    "# 11. MODEL COMPARISON SUMMARY\n",
    "# ======================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"{'Model':<30} | {'Accuracy':<10} | {'F1-Score':<10} | {'ROC-AUC':<10}\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "for name, res in model_results.items():\n",
    "    print(f\"{name:<30} | {res['accuracy']:<10.4f} | {res['f1']:<10.4f} | {res['roc_auc']:<10.4f}\")\n",
    "\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Find best model by F1-score\n",
    "best_model_name = max(model_results.items(), key=lambda x: x[1]['f1'])[0]\n",
    "print(f\"\\nBest model (by F1-score): {best_model_name}\")\n",
    "print(f\"  Accuracy: {model_results[best_model_name]['accuracy']:.4f}\")\n",
    "print(f\"  F1-Score: {model_results[best_model_name]['f1']:.4f}\")\n",
    "print(f\"  ROC-AUC:  {model_results[best_model_name]['roc_auc']:.4f}\")\n",
    "\n",
    "# ======================\n",
    "# 12. SAVE BEST MODEL\n",
    "# ======================\n",
    "\n",
    "# Save HistGradientBoosting as the best model (or save the best one if different)\n",
    "joblib.dump(hgb_pipeline, \"models/best_flight_delay_model.pkl\")\n",
    "print(\"\\nSaved HistGradientBoosting model to models/best_flight_delay_model.pkl\")\n",
    "print(\"(Note: You can modify this to save the best performing model instead)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28a1a8e7-b49a-43f8-bbd9-2356197178de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468896df-731f-4ebf-bef1-715042db32b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
