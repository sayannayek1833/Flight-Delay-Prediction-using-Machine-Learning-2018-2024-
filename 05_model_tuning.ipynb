{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42791a41",
   "metadata": {},
   "source": [
    "# 05 - Model Tuning & Comparison\n",
    "\n",
    "**Objective**: Perform hyperparameter tuning on selected models to optimize performance.\n",
    "\n",
    "**Approach**:\n",
    "- Use RandomizedSearchCV for efficient hyperparameter search\n",
    "- Focus on models with best baseline performance\n",
    "- Optimize for F1-score (balanced metric for imbalanced data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef1cb18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 120)\n",
    "sns.set()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c70a787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared for tuning\n",
      "Train shape: (465940, 32), Test shape: (116485, 32)\n",
      "\n",
      "============================================================\n",
      "XGBoost Hyperparameter Tuning\n",
      "============================================================\n",
      "Starting XGBoost tuning (this may take a while)...\n",
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, roc_auc_score\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Load engineered data\n",
    "data_path = \"data/flight_data_2018_2024_engineered.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "df.columns = df.columns.str.strip()\n",
    "\n",
    "# Prepare features (same as training notebook)\n",
    "target = \"DELAYED\"\n",
    "cols_to_remove = [\"DELAYED\", \"FlightDate\", \"Duplicate\", \"DivAirportLandings\", \"CRSArrTime\", \"ArrTimeBlk\"]\n",
    "cols_to_drop_final = [c for c in cols_to_remove if c in df.columns]\n",
    "\n",
    "X = df.drop(columns=cols_to_drop_final)\n",
    "y = df[target]\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Identify column types\n",
    "cat_cols = X.select_dtypes(include=\"object\").columns.tolist()\n",
    "num_cols = X.select_dtypes(exclude=\"object\").columns.tolist()\n",
    "low_card_cats = [c for c in cat_cols if df[c].nunique() <= 20]\n",
    "\n",
    "X_train_tune = X_train[num_cols + low_card_cats].copy()\n",
    "X_test_tune = X_test[num_cols + low_card_cats].copy()\n",
    "\n",
    "# Preprocessor\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False), low_card_cats),\n",
    "        (\"num\", \"passthrough\", num_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Data prepared for tuning\")\n",
    "print(f\"Train shape: {X_train_tune.shape}, Test shape: {X_test_tune.shape}\")\n",
    "\n",
    "# ======================\n",
    "# XGBoost Hyperparameter Tuning\n",
    "# ======================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"XGBoost Hyperparameter Tuning\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "xgb_param_dist = {\n",
    "    'model__n_estimators': [100, 200, 300],\n",
    "    'model__max_depth': [5, 7, 10, 15],\n",
    "    'model__learning_rate': [0.01, 0.1, 0.2],\n",
    "    'model__subsample': [0.8, 0.9, 1.0],\n",
    "    'model__colsample_bytree': [0.8, 0.9, 1.0],\n",
    "}\n",
    "\n",
    "xgb_pipeline = Pipeline([\n",
    "    (\"preprocessor\", preprocessor),\n",
    "    (\"model\", XGBClassifier(random_state=42, n_jobs=-1, eval_metric='logloss'))\n",
    "])\n",
    "\n",
    "xgb_search = RandomizedSearchCV(\n",
    "    xgb_pipeline, \n",
    "    param_distributions=xgb_param_dist,\n",
    "    n_iter=20, \n",
    "    cv=3, \n",
    "    scoring='f1', \n",
    "    n_jobs=-1, \n",
    "    random_state=42,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"Starting XGBoost tuning (this may take a while)...\")\n",
    "xgb_search.fit(X_train_tune, y_train)\n",
    "\n",
    "print(f\"\\nBest XGBoost parameters: {xgb_search.best_params_}\")\n",
    "print(f\"Best XGBoost CV F1-score: {xgb_search.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_xgb = xgb_search.best_estimator_.predict(X_test_tune)\n",
    "y_prob_xgb = xgb_search.best_estimator_.predict_proba(X_test_tune)[:, 1]\n",
    "\n",
    "print(f\"\\nTest set performance:\")\n",
    "print(f\"  Accuracy: {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(f\"  F1-score: {f1_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(f\"  ROC-AUC: {roc_auc_score(y_test, y_prob_xgb):.4f}\")\n",
    "\n",
    "# Save tuned model\n",
    "joblib.dump(xgb_search.best_estimator_, 'models/best_xgb_tuned_model.pkl')\n",
    "print(\"\\nTuned XGBoost model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1537754e-ee5b-439d-98e5-782d162b4233",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
